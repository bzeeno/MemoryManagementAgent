{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c77da1-8adc-4472-9f95-343ce2b94997",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [ ] Set up the tools\n",
    "- [ ] set up system prompt\n",
    "- [ ] implement check token warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eccd6c3-e54c-4a0f-a8f2-616642b98633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c78271-ec8e-4d06-b75d-0eef66c1c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define OpenAI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0340017-7657-4c11-8776-718c90defabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tools\n",
    "\n",
    "# Append to Context\n",
    "\n",
    "# Replace context\n",
    "\n",
    "# Search conversation history\n",
    "\n",
    "# Search conversation history based on date\n",
    "\n",
    "# Write to secondary memory\n",
    "\n",
    "# Read from secondary memory\n",
    "\n",
    "# Define functions to be binded to model\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61284256-ef48-40f2-a753-50b566fd6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prompt\n",
    "system_prompt = (\"\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6f06b-6930-4fc6-b304-ec5eb7f802e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and agent chain\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "mem_agent_chain = (\n",
    "    prompt\n",
    "    | model.bind_functions(functions)\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7abbde1-5075-48b9-a272-ea8179c23c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Agent State\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    return_to_agent: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08871107-508e-4791-abd0-28675d3f594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that determines whether to call function or go to checks\n",
    "def should_call_tool(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        state[\"return_to_agent\"] = false\n",
    "        return \"check token warning\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        state[\"return_to_agent\"] = true\n",
    "        return \"call tool\"\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}\n",
    "\n",
    "# Define function to check and handle token warning\n",
    "def check_token_warning(state):\n",
    "    # If token warning, add message\n",
    "\n",
    "def should_return_to_agent(state):\n",
    "    if state[\"return to agent\"] == true\n",
    "        return \"return to agent\"\n",
    "    else\n",
    "        return \"return to user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da35a0-93c7-410e-b7ad-0119ac77375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Define Nodes\n",
    "graph.add_node(\"mem agent\",  mem_agent_chain)\n",
    "graph.add_node(\"read/write memory\", call_tool)\n",
    "graph.add_node(\"check token warning\", check_token_warning)\n",
    "\n",
    "# Set entry point\n",
    "graph.set_entry_point(\"mem agent\")\n",
    "\n",
    "# Add conditional edge from agent to: read/write mem OR perform checks\n",
    "graph.add_conditional_edges(\"agent\", should_call_tool, {\"call tool\": \"read/write memory\", \"check token warning\": \"check token warning\"})\n",
    "\n",
    "# Add edge from tool call to check token warning\n",
    "graph.add_edge(\"read/write memory\", \"check token warning\")\n",
    "\n",
    "# Add conditional edge from perform checks to: Agent OR END\n",
    "graph.add_conditional_edges(\"check_token_warning\", should_return_to_agent, {\"return to agent\": \"agent\", \"return to user\": END) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
